{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Environment Setup"
      ],
      "metadata": {
        "id": "NIxaBg6pm4ff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ywk6vWusdhG",
        "outputId": "f26d4245-53a7-4ef0-84dd-cdd528cf778c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# # Mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "\n",
        "# Third-party\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "from torch.utils.data import Subset\n"
      ],
      "metadata": {
        "id": "mha5CRDknd5D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Kaggle API setup"
      ],
      "metadata": {
        "id": "IK8emNblnFce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "\n",
        "# Upload kaggle.json and configure Kaggle API\n",
        "from google.colab import files\n",
        "files.upload()  # Upload kaggle.json manually\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "u6I5WkK6nECu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and unzip dataset from Kaggle\n",
        "!mkdir -p /content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd\n",
        "!kaggle datasets download -d ismailnasri20/driver-drowsiness-dataset-ddd -p /content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd --unzip\n"
      ],
      "metadata": {
        "id": "KdGCKGeOnraM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading & Inspection"
      ],
      "metadata": {
        "id": "wARi6pg9n6vM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PsIwsjRASwe7"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/Driver Drowsiness Dataset (DDD)/'\n",
        "\n",
        "def show_first_5_images(folder_name):\n",
        "    folder_path = os.path.join(dataset_path, folder_name)\n",
        "    image_files = os.listdir(folder_path)[:5]  # first 5 image filenames\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, image_file in enumerate(image_files):\n",
        "        img_path = os.path.join(folder_path, image_file)\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"{folder_name} {i+1}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Show first 5 images from 'drowsy' folder\n",
        "show_first_5_images('Drowsy')\n",
        "\n",
        "# Show first 5 images from 'notdrowsy' folder\n",
        "show_first_5_images('Non Drowsy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Quality Check"
      ],
      "metadata": {
        "id": "zgDWkRrgoGtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqZ2BSVSWWfi"
      },
      "outputs": [],
      "source": [
        "def check_images(root_dir):\n",
        "    for root, _, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            path = os.path.join(root, file)\n",
        "            try:\n",
        "                img = Image.open(path)\n",
        "                img.verify()\n",
        "            except Exception:\n",
        "                print(f\"Corrupted: {path}\")\n",
        "                # Optional: os.remove(path)\n",
        "\n",
        "check_images(\"/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/Driver Drowsiness Dataset (DDD)/\")#B0084.png was removed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transform, Dataset Split"
      ],
      "metadata": {
        "id": "Cq3AxRyPoMxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/Driver Drowsiness Dataset (DDD)/'\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "total_size = len(full_dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = Subset(train_dataset, range(1000))\n",
        "val_dataset = Subset(val_dataset, range(200))\n",
        "\n",
        "\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "2rUpkiR0Jcl2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Function"
      ],
      "metadata": {
        "id": "-zGYd1WmpgdS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3wdwdV4qj5YF"
      },
      "outputs": [],
      "source": [
        "def train_fn(index, total_epochs=10):\n",
        "\n",
        "    global train_dataset, val_dataset\n",
        "    train_dataset = Subset(train_dataset, range(1000))\n",
        "    val_dataset = Subset(val_dataset, range(200))\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "    for param in model.fc.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    save_dir = \"/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/checkpoints\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    checkpoint_path = os.path.join(save_dir, f\"resnet50_gpu_{index}_checkpoint.pth\")\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_acc = 0.0\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_acc = checkpoint.get('best_acc', 0.0)\n",
        "        print(f\"[gpu:{index}] Resuming from epoch {start_epoch} with best acc {best_acc:.4f}\")\n",
        "\n",
        "    for epoch in range(start_epoch, total_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                print(f\"[gpu:{index}] Epoch {epoch+1} | Batch {i}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        epoch_loss = total_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        print(f\"[gpu:{index}] Epoch {epoch+1} - Train Loss: {epoch_loss:.4f} - Train Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_epoch_loss = val_loss / val_total\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        print(f\"[gpu:{index}] Epoch {epoch+1} - Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_acc': max(best_acc, val_epoch_acc)\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        if val_epoch_acc > best_acc:\n",
        "            best_acc = val_epoch_acc\n",
        "            best_model_path = os.path.join(save_dir, f\"best_model_gpu_{index}.pth\")\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"[gpu:{index}] Best model saved with acc {best_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Training"
      ],
      "metadata": {
        "id": "MHAkMqWpplKB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "aP8KSPxckIUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac35c6d-d981-4ce2-800a-01512c9e2dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[gpu:0] Resuming from epoch 2 with best acc 0.7600\n",
            "[gpu:0] Epoch 3 | Batch 0/32 | Loss: 0.6500\n",
            "[gpu:0] Epoch 3 | Batch 20/32 | Loss: 0.6664\n",
            "[gpu:0] Epoch 3 - Train Loss: 0.6271 - Train Acc: 0.7680\n",
            "[gpu:0] Epoch 3 - Val Loss: 0.6206 - Val Acc: 0.8200\n",
            "[gpu:0] Best model saved with acc 0.8200\n",
            "[gpu:0] Epoch 4 | Batch 0/32 | Loss: 0.6297\n",
            "[gpu:0] Epoch 4 | Batch 20/32 | Loss: 0.5927\n",
            "[gpu:0] Epoch 4 - Train Loss: 0.6066 - Train Acc: 0.7820\n",
            "[gpu:0] Epoch 4 - Val Loss: 0.5950 - Val Acc: 0.8350\n",
            "[gpu:0] Best model saved with acc 0.8350\n",
            "[gpu:0] Epoch 5 | Batch 0/32 | Loss: 0.6085\n",
            "[gpu:0] Epoch 5 | Batch 20/32 | Loss: 0.6044\n",
            "[gpu:0] Epoch 5 - Train Loss: 0.5841 - Train Acc: 0.8300\n",
            "[gpu:0] Epoch 5 - Val Loss: 0.5759 - Val Acc: 0.8700\n",
            "[gpu:0] Best model saved with acc 0.8700\n",
            "[gpu:0] Epoch 6 | Batch 0/32 | Loss: 0.5946\n",
            "[gpu:0] Epoch 6 | Batch 20/32 | Loss: 0.5758\n",
            "[gpu:0] Epoch 6 - Train Loss: 0.5673 - Train Acc: 0.8390\n",
            "[gpu:0] Epoch 6 - Val Loss: 0.5643 - Val Acc: 0.8550\n",
            "[gpu:0] Epoch 7 | Batch 0/32 | Loss: 0.5682\n",
            "[gpu:0] Epoch 7 | Batch 20/32 | Loss: 0.5424\n",
            "[gpu:0] Epoch 7 - Train Loss: 0.5496 - Train Acc: 0.8560\n",
            "[gpu:0] Epoch 7 - Val Loss: 0.5431 - Val Acc: 0.8800\n",
            "[gpu:0] Best model saved with acc 0.8800\n",
            "[gpu:0] Epoch 8 | Batch 0/32 | Loss: 0.5599\n",
            "[gpu:0] Epoch 8 | Batch 20/32 | Loss: 0.5342\n",
            "[gpu:0] Epoch 8 - Train Loss: 0.5342 - Train Acc: 0.8580\n",
            "[gpu:0] Epoch 8 - Val Loss: 0.5348 - Val Acc: 0.8850\n",
            "[gpu:0] Best model saved with acc 0.8850\n",
            "[gpu:0] Epoch 9 | Batch 0/32 | Loss: 0.5455\n",
            "[gpu:0] Epoch 9 | Batch 20/32 | Loss: 0.4786\n",
            "[gpu:0] Epoch 9 - Train Loss: 0.5170 - Train Acc: 0.8820\n",
            "[gpu:0] Epoch 9 - Val Loss: 0.5161 - Val Acc: 0.9050\n",
            "[gpu:0] Best model saved with acc 0.9050\n",
            "[gpu:0] Epoch 10 | Batch 0/32 | Loss: 0.5348\n",
            "[gpu:0] Epoch 10 | Batch 20/32 | Loss: 0.5082\n",
            "[gpu:0] Epoch 10 - Train Loss: 0.5015 - Train Acc: 0.8820\n",
            "[gpu:0] Epoch 10 - Val Loss: 0.4987 - Val Acc: 0.8950\n"
          ]
        }
      ],
      "source": [
        "train_fn(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation\n"
      ],
      "metadata": {
        "id": "4atwDyf-c-gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_best_model(index):\n",
        "    from torchvision.models import resnet50, ResNet50_Weights\n",
        "    import torch.nn as nn\n",
        "    import torch\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)  # Match your output layer\n",
        "    model = model.to(device)\n",
        "\n",
        "    best_model_path = f\"/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/checkpoints/best_model_gpu_{index}.pth\"\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    return model.to(device)\n"
      ],
      "metadata": {
        "id": "HuA2pyt-c__X"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "id": "_O3cHu0VdGxp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_best_model(index=0)\n"
      ],
      "metadata": {
        "id": "epro1bv7dedV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "evaluate(model, test_loader, device)"
      ],
      "metadata": {
        "id": "zPtUxY8ideQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Deployment"
      ],
      "metadata": {
        "id": "WxTl_gjTeb1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your best model\n",
        "model = load_best_model(index=0)\n",
        "model.eval()\n",
        "\n",
        "# Example input to trace model structure\n",
        "example_input = torch.randn(1, 3, 224, 224).to(next(model.parameters()).device)\n",
        "\n",
        "# Convert to TorchScript\n",
        "traced_script_module = torch.jit.trace(model, example_input)\n",
        "\n",
        "export_path = \"/content/drive/MyDrive/kaggle_datasets/driver-drowsiness-dataset-ddd/deployed-models\"\n",
        "\n",
        "# TorchScript export\n",
        "traced_script_module.save(f\"{export_path}/resnet50_finetuned_script.pt\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NTWCuEYydsO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "IK8emNblnFce",
        "WxTl_gjTeb1Q"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}